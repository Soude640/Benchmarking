# -*- coding: utf-8 -*-
"""XGBoost classifier_with_iris_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gTUNndM1_x--RZrnkF82p0Cfv0CW3-KH
"""

import numpy as np
import pandas as pd 
from sklearn.datasets import load_iris
from xgboost import plot_importance
from matplotlib import pyplot as plt
from sklearn.metrics import accuracy_score
import xgboost as xgb
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import roc_auc_score

!pip install pyspark

from pyspark import SparkContext, SparkConf
from pyspark.sql import functions as F
from pyspark.sql import Window, SparkSession
from pyspark.sql.functions import col
from pyspark.sql.types import DoubleType, StringType, ArrayType, LongType, FloatType, DateType
from sklearn.model_selection import train_test_split
import sys
import xgboost as xgb
import numpy as np
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as plt
from sklearn.preprocessing import label_binarize
import lightgbm as lgb

spark = (SparkSession.builder
                  .appName('XGB')
                  .enableHiveSupport()
                  .config("spark.executor.memory", "10G")
                  .config("spark.driver.memory","5G")
                  .config("spark.executor.cores","7")
                  .config("spark.python.worker.memory","4G")
                  .config("spark.driver.maxResultSize","0")
                  .config("spark.sql.crossJoin.enabled", "true")
                  .config("spark.serializer","org.apache.spark.serializer.KryoSerializer")
                  .config("spark.default.parallelism","2")
                  .config("spark.kryoserializer.buffer.max.mb", "2047").getOrCreate()
        )

iris = load_iris()
X,y = iris.data,iris.target
col = iris.target_names
train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=1)   
parameters = {
              'max_depth': [5, 10, 15],
              'learning_rate': [0.01, 0.02],
              'n_estimators': [500, 1000],
              'min_child_weight': [0, 2, 5, 10, 20],
              #'max_delta_step': [0, 0.2, 0.6, 1, 2],
              #'subsample': [0.6, 0.7, 0.8, 0.85, 0.95],
              #'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9],
              #'reg_alpha': [0, 0.25, 0.5, 0.75, 1],
              #'reg_lambda': [0.2, 0.4, 0.6, 0.8, 1],
              #'scale_pos_weight': [0.2, 0.4, 0.6, 0.8, 1]

}

xlf = xgb.XGBClassifier(max_depth=10,
            learning_rate=0.01,
            n_estimators=2000,
            silent=True,
            objective='multi:softmax',
            num_class=3 ,
            nthread=-1,
            gamma=0,
            min_child_weight=1,
            max_delta_step=0,
            subsample=0.85,
            colsample_bytree=0.7,
            colsample_bylevel=1,
            reg_alpha=0,
            reg_lambda=1,
            scale_pos_weight=1,
            seed=0,
            missing=1)

gs = GridSearchCV(xlf, param_grid=parameters, scoring='accuracy', cv=3)
gs.fit(train_x, train_y)

print("Best score: %0.3f" % gs.best_score_)
print("Best parameters set: %s" % gs.best_params_ )

# make predictions for test data
y_pred = gs.predict(test_x)
predictions = [round(value) for value in y_pred]
# evaluate predictions
accuracy = accuracy_score(test_y, predictions)
print("Accuracy: %.2f%%" % (accuracy * 100.0))





